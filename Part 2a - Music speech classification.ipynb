{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music vs. Speech Classification with Deep Learning\n",
    "\n",
    "This tutorial shows how different Convolutional Neural Network architectures are used for the taks of discriminating a piece of audio whether it is music or speech (binary classification).\n",
    "\n",
    "The data set used is the [Music Speech](http://marsyasweb.appspot.com/download/data_sets/) data set compiled by George Tzanetakis. It consists of 128 tracks, each 30 seconds long. Each class (music/speech) has 64 examples. The tracks are all 22050Hz Mono 16-bit audio files in .wav format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "This tutorial contains:\n",
    "* Loading and Preprocessing of Audio files\n",
    "* Loading class files from CSV and using Label Encoder\n",
    "* Generating Mel spectrograms\n",
    "* Standardization of Data\n",
    "* Convolutional Neural Networks: single, stacked, parallel\n",
    "* ReLU Activation\n",
    "* Dropout\n",
    "* Train/Test set split\n",
    "* (Cross-validation - TODO)\n",
    "\n",
    "You can execute the following code blocks by pressing SHIFT+Enter consecutively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "The data set can be downloaded from [here](http://opihi.cs.uvic.ca/sound/music_speech.tar.gz).\n",
    "\n",
    "Please unpack it (it is .tar.gz compressed)\n",
    "\n",
    "Set the path to the unpacke folder in the next box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = '/data/music/GTZAN_music_speech'\n",
    "AUDIO_PATH = os.path.join(DATA_PATH, 'wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd # Pandas for reading CSV files and easier Data handling in preparation\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports for audio reading and processing\n",
    "import audio_spectrogram as rp\n",
    "from audiofile_read import audiofile_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Metadata\n",
    "\n",
    "The tab-separated file contains pairs of filename TAB class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>speech/stupid.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/teachers2.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/danie1.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/oneday.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/jvoice.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/relation.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/geography.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/pulp2.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/greek1.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech/conversion.wav</th>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            1\n",
       "0                            \n",
       "speech/stupid.wav      speech\n",
       "speech/teachers2.wav   speech\n",
       "speech/danie1.wav      speech\n",
       "speech/oneday.wav      speech\n",
       "speech/jvoice.wav      speech\n",
       "speech/relation.wav    speech\n",
       "speech/geography.wav   speech\n",
       "speech/pulp2.wav       speech\n",
       "speech/greek1.wav      speech\n",
       "speech/conversion.wav  speech"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = os.path.join(DATA_PATH,'filelist_wav_relpath_wclasses.txt')\n",
    "metadata = pd.read_csv(csv_file, index_col=0, sep='\\t', header=None)\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two lists: one with filenames and one with associated classes\n",
    "filelist = metadata.index.tolist()\n",
    "classes = metadata[1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Labels to Numbers\n",
    "\n",
    "String labels need to be encoded as numbers. We use the LabelEncoder from the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speech', 'speech', 'speech', 'speech', 'speech']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 classes: music, speech\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(classes)\n",
    "print(len(labelencoder.classes_), \"classes:\", \", \".join(list(labelencoder.classes_)))\n",
    "\n",
    "classes_num = labelencoder.transform(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first few classes (speech)\n",
    "classes_num[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first few classes (music)\n",
    "classes_num[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In order to correctly re-transform any predicted numbers into strings, we keep the labelencoder for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................\n",
      "Read 128 audio files\n"
     ]
    }
   ],
   "source": [
    "list_spectrograms = [] # spectrograms are put into a list first\n",
    "\n",
    "# desired output parameters\n",
    "n_mel_bands = 40   # y axis\n",
    "frames = 80        # x axis\n",
    "\n",
    "# some FFT parameters\n",
    "fft_window_size=512\n",
    "fft_overlap = 0.5\n",
    "hop_size = int(fft_window_size*(1-fft_overlap))\n",
    "segment_size = fft_window_size + (frames-1) * hop_size # segment size for desired # frames\n",
    "\n",
    "for filename in filelist:\n",
    "    print(\".\", end='')\n",
    "    filepath = os.path.join(AUDIO_PATH, filename)\n",
    "    samplerate, samplewidth, wavedata = audiofile_read(filepath,verbose=False)\n",
    "    sample_length = wavedata.shape[0]\n",
    "\n",
    "    # make Mono (in case of multiple channels / stereo)\n",
    "    if wavedata.ndim > 1:\n",
    "        wavedata = np.mean(wavedata, 1)\n",
    "        \n",
    "    # take only a segment; choose start position:\n",
    "    #pos = 0 # beginning\n",
    "    pos = int(wavedata.shape[0]/2 - segment_size/2) # center minus half segment size\n",
    "    wav_segment = wavedata[pos:pos+segment_size]\n",
    "\n",
    "    # 1) FFT spectrogram \n",
    "    spectrogram = rp.calc_spectrogram(wav_segment,fft_window_size,fft_overlap)\n",
    "\n",
    "    # 2) Transform to perceptual Mel scale (uses librosa.filters.mel)\n",
    "    spectrogram = rp.transform2mel(spectrogram,samplerate,fft_window_size,n_mel_bands)\n",
    "        \n",
    "    # 3) Log 10 transform\n",
    "    spectrogram = np.log10(spectrogram)\n",
    "    \n",
    "    list_spectrograms.append(spectrogram)\n",
    "        \n",
    "print(\"\\nRead\", len(filelist), \"audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00320435, -0.05340576, -0.09249878, ...,  0.14624023,\n",
       "        0.19299316,  0.18325806])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 80)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An audio segment is 0.94 seconds long\n"
     ]
    }
   ],
   "source": [
    "print(\"An audio segment is\", round(float(segment_size) / samplerate, 2), \"seconds long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For simplicity of this tutorial, here we load only 1 single segment of ~ 1 second length from each audio file.\n",
    "In a real setting, one would create training instances of as many audio segments as possible to be fed to a Neural Network.\n",
    "\n",
    "### Show Waveform and Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bda8832822cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# you can skip this if you do not have matplotlib installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# you can skip this if you do not have matplotlib installed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# show 1 sec wave segment\n",
    "plt.plot(wav_segment)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show spectrogram\n",
    "fig = plt.imshow(spectrogram, origin='lower')\n",
    "fig.set_cmap('jet')\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make 1 big array of list of spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of many 40x80 spectrograms is made into 1 big array\n",
    "# config.floatX is from Theano configration to enforce float32 precision (needed for GPU computation)\n",
    "data = np.array(list_spectrograms, dtype=config.floatX)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "As in the Car image tutorial we use <b>Zero-mean Unit-variance standardization</b> (also known as Z-score normalization).\n",
    "However, this time we use <b>attribute-wise standardization</b>, i.e. each pixel is standardized individually, as opposed to computing a single mean and single standard deviation of all values.\n",
    "\n",
    "('Flat' standardization would also be possible, but we have seen benefits of attribut-wise standardization in our experiments).\n",
    "\n",
    "This time, we use the StandardScaler from the scikit-learn package for our purpose.\n",
    "As it works typically on vector data, we have to vectorize (i.e. reshape) our matrices first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "N, ydim, xdim = data.shape\n",
    "data = data.reshape(N, xdim*ydim)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "scaler = preprocessing.StandardScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show mean and standard deviation: two vectors with same length as data.shape[1]\n",
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train & Test Set \n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%).\n",
    "\n",
    "Here we compare Random Split vs. Stratified Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_size = 0.25 # % portion of whole data set to keep for testing, i.e. 75% is used for training\n",
    "\n",
    "# Normal (random) split of data set into 2 parts\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set, train_classes, test_classes = train_test_split(data, classes_num, test_size=testset_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two classes may be unbalanced\n",
    "print \"Class Counts: Class 0:\", sum(train_classes==0), \"Class 1:\", sum(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better: Stratified Split retains the class balance in both sets\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "if sklearn_version == '0.17':\n",
    "    splits = StratifiedShuffleSplit(classes_num, n_iter=1, test_size=testset_size, random_state=0)\n",
    "elif sklearn_version == '0.18':\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "    splits = splitter.split(data, classes_num)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    print \"TRAIN INDEX:\", train_index\n",
    "    print \"TEST INDEX:\", test_index\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes_num[train_index]\n",
    "    test_classes = classes_num[test_index]\n",
    "# Note: this for loop is only executed once, if n_iter==1 resp. n_splits==1\n",
    "\n",
    "print train_set.shape\n",
    "print test_set.shape\n",
    "# Note: we will reshape the data later back to matrix form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Class Counts: Class 0:\", sum(train_classes==0), \"Class 1:\", sum(train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images or spectrograms, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which aggregates neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "### Adding the channel\n",
    "\n",
    "As previously in the Car image tutorial, we need to add a dimension for the color channel to the data. RGB images typically have an 3rd dimension with the color. \n",
    "\n",
    "<b>Spectrograms, however, are considered like greyscale images, as in the previous tutorial.\n",
    "Likewise we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "<i>Same as in the previous tutorial:</i>\n",
    "\n",
    "In Theano, traditionally the color channel was the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured now in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" with \"tf\" (Tensorflow) being the default image ordering even though you use Theano. Depending on this, use one of the code lines below.\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1 # for grey-scale, 3 for RGB, but usually already present in the data\n",
    "\n",
    "if keras.backend.image_dim_ordering() == 'th':\n",
    "    # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "    train_set = train_set.reshape(train_set.shape[0], n_channels, ydim, xdim)\n",
    "    test_set = test_set.reshape(test_set.shape[0], n_channels, ydim, xdim)\n",
    "else:\n",
    "    # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "    train_set = train_set.reshape(train_set.shape[0], ydim, xdim, n_channels)\n",
    "    test_set = test_set.reshape(test_set.shape[0], ydim, xdim, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "input_shape = train_set.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network Models in Keras\n",
    "\n",
    "## Sequential Models\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Single Layer and a Two Layer CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try: (comment/uncomment code in the following code block)\n",
    "* 1 Layer\n",
    "* 2 Layer\n",
    "* more conv_filters\n",
    "* Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(0) # make results repeatable\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "conv_filters = 32   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, 3, 3, input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, 3, 3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# After Convolution, we have a 16*x*y matrix output\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "# Note: Keras does automatic shape inference, i.e. it knows how many (flat) input units the next layer will need,\n",
    "# so no parameter is needed for the Flatten() layer.\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256, activation='sigmoid')) \n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy goes up pretty quickly for 1 layer on Train set! Also on Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# always execute this, and then a box of accuracy_score below to print the result\n",
    "test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 layer\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer + 32 convolution filters\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer + 32 convolution filters + Dropout\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Parameters & Techniques\n",
    "\n",
    "Try out more parameters and techniques: (comment/uncomment code blocks below)\n",
    "* Adding ReLU activation\n",
    "* Adding Batch normalization\n",
    "* Adding Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "conv_filters = 16   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, 3, 3, border_mode='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Convolution2D(conv_filters, 3, 3, border_mode='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(256))  \n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIREX 2015 model\n",
    "model = Sequential()\n",
    "\n",
    "conv_filters = 15   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "model.add(Convolution2D(conv_filters, 12, 8, border_mode='valid', input_shape=input_shape))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "model.add(Activation('sigmoid')) \n",
    "model.add(MaxPooling2D(pool_size=(2, 1))) \n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "model.add(Flatten()) \n",
    "\n",
    "# Full layer\n",
    "model.add(Dense(200, activation='sigmoid'))  \n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "# For binary/2-class problems use ONE sigmoid unit, \n",
    "# for multi-class/multi-label problems use n output units and activation='softmax!'\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and training the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying Accuracy on Test Set\n",
    "\n",
    "test_pred = model.predict_classes(test_set)\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parallel CNNs\n",
    "\n",
    "It has been discovered, that CNNs for music work best, when they have one filter that is detecting frequencies in the vertical axis, and nother filter that is focused on the time axis, i.e. detecting rhythm. Consequently, this is realized in a parallel CNN, where 2 layers are not stacked after each other, but first run independently in parallel with their output being merged later.\n",
    "\n",
    "To create parallel CNNs we need a \"graph-based\" model. In Keras 1.x this is realized via the functional API of the Model() class.\n",
    "We use it to create two CNN layers that run in parallel to each other and are merged subsequently.\n",
    "In the functional API, you pass the name of the previous layer in (brackets) after defining the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input only specifies the input shape\n",
    "input = Input(input_shape)\n",
    "\n",
    "# CNN layers\n",
    "# specify desired number of filters\n",
    "n_filters = 16 \n",
    "# The functional API allows to specify the predecessor in (brackets) after the new Layer function call\n",
    "conv_layer1 = Convolution2D(16, 10, 2)(input)  # a vertical filter\n",
    "conv_layer2 = Convolution2D(25, 2, 10)(input)  # a horizontal filter\n",
    "\n",
    "# possibly add Activation('relu') here\n",
    "\n",
    "# Pooling layers\n",
    "maxpool1 = MaxPooling2D(pool_size=(1,2))(conv_layer1) # horizontal pooling\n",
    "maxpool2 = MaxPooling2D(pool_size=(2,1))(conv_layer2) # vertical pooling\n",
    "\n",
    "# we have to flatten the Pooling output in order to be concatenated\n",
    "poolflat1 = Flatten()(maxpool1)\n",
    "poolflat2 = Flatten()(maxpool2)\n",
    "\n",
    "# Merge the 2\n",
    "merged = merge([poolflat1, poolflat2], mode='concat')\n",
    "\n",
    "full = Dense(256, activation='relu')(merged)\n",
    "output_layer = Dense(1, activation='sigmoid')(full)\n",
    "\n",
    "# finally create the model\n",
    "model = Model(input=input, output=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'binary_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_set, train_classes, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set\n",
    "\n",
    "Note: The functional API, i.e. Model() does not have a convenience method `.predict_classes()`. We therefore do 'raw' predictions with `predict()`, which returns values between 0 and 1, and then round to the nearest value (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_set)\n",
    "test_pred[0:35,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.round(test_pred)\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network: LSTM\n",
    "\n",
    "For an RNN we define the number of time steps to be considered in a sequence. Here, we take a sequence of 10 audio samples from each audio file. The data has to be aligned accordingly to reflect the time steps, we do that in the reshape step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 10\n",
    "list_spectrograms = [] # spectrograms are put into a list first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all audio files again, as before we kept only 1 segment and now we process 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filelist:\n",
    "    print \".\", \n",
    "    filepath = os.path.join(path, filename)\n",
    "    samplerate, samplewidth, wavedata = audiofile_read(filepath,verbose=False)\n",
    "    sample_length = wavedata.shape[0]\n",
    "\n",
    "    # make Mono (in case of multiple channels / stereo)\n",
    "    if wavedata.ndim > 1:\n",
    "        wavedata = np.mean(wavedata, 1)\n",
    "        \n",
    "    # HERE WE TAKE MULTIPLE SEGMENTS NOW\n",
    "    pos = 0 # start position of segment\n",
    "    \n",
    "    for t in range(timesteps):\n",
    "        wav_segment = wavedata[pos:pos+segment_size]\n",
    "\n",
    "        # 1) FFT spectrogram \n",
    "        spectrogram = rp.calc_spectrogram(wav_segment,fft_window_size,fft_overlap)\n",
    "\n",
    "        # 2) Transform to perceptual Mel scale (uses librosa.filters.mel)\n",
    "        spectrogram = rp.transform2mel(spectrogram,samplerate,fft_window_size,n_mel_bands)\n",
    "\n",
    "        # 3) Log 10 transform\n",
    "        spectrogram = np.log10(spectrogram)\n",
    "\n",
    "        # add to lists\n",
    "        list_spectrograms.append(spectrogram)\n",
    "        \n",
    "         # jump forward exactly by 1 segment\n",
    "        pos += segment_size   # TODO check if not surpassing file length\n",
    "        \n",
    "print \"\\nRead\", len(filelist), \"audio files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes_num_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 1 big array\n",
    "data = np.array(list_spectrograms, dtype=config.floatX)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "N, ydim, xdim = data.shape\n",
    "data = data.reshape(N, xdim*ydim)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "scaler = preprocessing.StandardScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping for RNN\n",
    "\n",
    "Now, the second dimension ist not the channel (as in a CNN), but the number of time steps in a sequence to be processed.\n",
    "\n",
    "We reshape accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = data.shape[0]/timesteps  # we had N instances (* timesteps)\n",
    "data = data.reshape(N, timesteps, -1)   # -1 means take the remaining dimensions from the data size given\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data.shape[2]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative to input dim\n",
    "input_shape = data.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Test Set\n",
    "\n",
    "We kept the train_index and test_index from above, and essentially split identically as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data[train_index]\n",
    "test_set = data[test_index]\n",
    "train_classes = classes_num[train_index]\n",
    "test_classes = classes_num[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# LSTM layer with 256 units\n",
    "model.add(LSTM(output_dim=128, input_length=timesteps, input_dim=input_dim, activation='tanh', \n",
    "               return_sequences=True))\n",
    "model.add(LSTM(output_dim=128, input_length=timesteps, input_dim=input_dim, activation='tanh', \n",
    "               return_sequences=False, dropout_W=0.25, dropout_U=0.25))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer with 1 unit\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for RNNs take RMSprop as the optimizer!\n",
    "optimizer='rmsprop'\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "epochs = 15\n",
    "validation_data = (test_set, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_set, train_classes, batch_size=16, nb_epoch=epochs, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Test Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict_classes(test_set)\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative\n",
    "score = model.evaluate(test_set, test_classes, batch_size=16)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Extension 1:\n",
    "# other optimizers:\n",
    "# from keras.optimizers import SGD, RMSprop, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Extension 2:\n",
    "# 2-layer CNN stacked RNN\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# TODO add 2-layer CNN before RNN\n",
    "# for stacked CNN + RNN we need keras.layers.wrappers.TimeDistributed(layer)\n",
    "# conv_filters = 32   # number of convolution filters (= CNN depth)\n",
    "\n",
    "# Layer 1\n",
    "#model.add(Convolution2D(conv_filters, 3, 3, input_shape=input_shape))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "#model.add(Dropout(0.25)) \n",
    "\n",
    "# Layer 2\n",
    "#model.add(Convolution2D(conv_filters, 3, 3))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "\n",
    "# In order to feed this to a Full(Dense) layer, we need to flatten all data\n",
    "#model.add(Flatten()) \n",
    "\n",
    "# LSTM layer with 256 units\n",
    "model.add(LSTM(output_dim=128, input_length=timesteps, input_dim=input_dim, activation='tanh', \n",
    "               return_sequences=False, dropout_W=0.25, dropout_U=0.25,\n",
    "               inner_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer with 1 unit\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
